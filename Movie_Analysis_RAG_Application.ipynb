{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**API Key**"
      ],
      "metadata": {
        "id": "LmlDJgkwyWTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"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\""
      ],
      "metadata": {
        "id": "_E4aDbES1JrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installation**"
      ],
      "metadata": {
        "id": "a65zJ9UVSuPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "id": "BYJgpsLj11Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "pMOjFwZs7wvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "id": "wNjoIIek7_yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using  **LLAMA Model**  and  **API Key**  for Chat with Application"
      ],
      "metadata": {
        "id": "f6arbET2SxqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key= API_KEY,\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "JL6KGEY02FQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading **Trained Dataset** by using Pandas"
      ],
      "metadata": {
        "id": "fA58vOWdToPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Movies Dataset:** https://docs.google.com/spreadsheets/d/1RXRuXlni1vzXgm0Egl6MysTHMM2zJR7bl8TKTRGnxfQ/edit?usp=sharing"
      ],
      "metadata": {
        "id": "-UqEccS-UZou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset into a DataFrame\n",
        "pd = pd.read_csv(\"/content/Hydra-Movie-Scrape - Hydra-Movie-Scrape (1).csv\")\n",
        "\n",
        "# Use the head() method on the DataFrame to display the first 6 rows\n",
        "df = pd.head(6)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "r7g7UeK64J7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Application Generation**"
      ],
      "metadata": {
        "id": "jqnT4h2gUeSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from groq import Groq\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Step 1: Initialize the Groq API client with your API key\n",
        "\n",
        "client = Groq(api_key=API_KEY)\n",
        "\n",
        "# Step 2: Load your dataset\n",
        "data = pd.read_csv(\"/content/Hydra-Movie-Scrape - Hydra-Movie-Scrape (1).csv\")\n",
        "\n",
        "# Step 3: Handle missing values in the \"Summary\" column by replacing NaN with empty strings\n",
        "data['Summary'].fillna('', inplace=True)\n",
        "\n",
        "# Step 4: Initialize the embedding model\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Step 5: Embed the \"Summary\" column\n",
        "embeddings = embedder.encode(data['Summary'].tolist(), show_progress_bar=True)\n",
        "\n",
        "# Step 6: Store the embeddings as a list of arrays in the DataFrame\n",
        "data['summary_embeddings'] = list(embeddings)\n",
        "\n",
        "# Step 7: Initialize FAISS index for vector search\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings))  # Add embeddings to FAISS\n",
        "\n",
        "# Step 8: Define a function to search for similar summaries using FAISS\n",
        "def search_similar_summaries(query, top_n=5):\n",
        "    query_embedding = embedder.encode([query])\n",
        "    distances, indices = index.search(query_embedding, top_n)\n",
        "    return data.iloc[indices[0]]\n",
        "\n",
        "# Step 9: Define a function to generate the augmented response using the Groq API\n",
        "def generate_response(user_query):\n",
        "    # Retrieve similar movie summaries from the dataset\n",
        "    retrieved_data = search_similar_summaries(user_query, top_n=5)\n",
        "\n",
        "    # Create context from the retrieved data to pass as input to the Groq API\n",
        "    context = \"\"\n",
        "    for idx, row in retrieved_data.iterrows():\n",
        "        context += f\"Title: {row['Title']}\\nSummary: {row['Summary']}\\nGenres: {row['Genres']}\\nDirector: {row['Director']}\\n\\n\"\n",
        "\n",
        "    # Construct the final prompt for Groq's language model\n",
        "    prompt = f\"The user asked: {user_query}\\nHere are some relevant movies:\\n{context}\"\n",
        "\n",
        "    # Use the Groq API to get a completion from the language model\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama3-8b-8192\",\n",
        "    )\n",
        "\n",
        "    # Access the generated response using dot notation\n",
        "    generated_text = chat_completion.choices[0].message.content\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "# Step 10: Example usage of the RAG application\n",
        "user_query = \"Man of steel\"\n",
        "response = generate_response(user_query)\n",
        "\n",
        "# Output the generated response\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "sOCn8_436tTk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}